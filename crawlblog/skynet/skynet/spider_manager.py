import json# import scrapy.crawler.Crawlerimport threading,multiprocessingimport timefrom scrapy.crawler import CrawlerProcess, CrawlerRunner# from scrapy.utils import reactor# from twisted.internet.iocpreactor import reactorfrom twisted.internet import reactor, deferfrom skynet.skynet.spiders.ocean_engine_spider import OceanengineSpiderfrom skynet.skynet.spiders.pablog_spider import CblogsSpiderimport pika# import gevent# from gevent import monkey; monkey.patch_all()# from tutorial.tutorial import settingsimport syssys.path.append(".settings")from scrapy.utils.project import get_project_settings# s = get_project_settings()# print("settings", s)# from .pablog import *from skynet.skynet.interactors import dict2taskfrom skynet.skynet.interactors import dict2spdinrimport scrapyimport scrapy.crawler as crawlerfrom multiprocessing import Process, Queuefrom twisted.internet import reactorclass SpiderManager(object):    def __init__(self):        """这个地方的可以构建消息队列的订阅，配置订阅消息是否默认ACK，等等"""        # thread 触发consumer mock  rabbitmq api  redis subpulish        self.process = CrawlerRunner(            settings={'LOG_FORMAT': '%(levelname)s: %(message)s', 'LOG_LEVEL': 'WARN', 'LOG_FILE': 'log.txt'})        credentials = pika.PlainCredentials(username="dev-spider", password="dev-spider")        connection = pika.BlockingConnection(            pika.ConnectionParameters(host='192.168.50.221', port=5672, credentials=credentials, virtual_host='spider'))        channel = connection.channel()        print('[*] Waiting for logs. To exit press CTRL+C')        channel.basic_qos(prefetch_count=10, global_qos=True)  # 缓冲区的数量        channel.basic_consume(on_message_callback=self.callback, queue="dev-spider1", auto_ack=False)        channel.start_consuming()        # {"crawl":"OceanengineSpider","task_name":"try1", "taskid":1, "infoid":1}    def convert(self, body):        arg = json.loads(body)        # print("arg",arg,"type",type(arg),"body",body,type(body),'arg["crawl"]',arg["crawl"])        dict = {"OceanengineSpider": OceanengineSpider, "CblogsSpider": CblogsSpider}        return dict[arg["crawl"]]    def crawl(self,ch,method,body):        # process = CrawlerProcess(settings = {'LOG_FORMAT': '%(levelname)s: %(message)s', 'LOG_LEVEL': 'WARN', 'LOG_FILE': 'log.txt'})        arg = json.loads(body)        runner = CrawlerProcess(settings = {'LOG_FORMAT': '%(levelname)s: %(message)s', 'LOG_LEVEL': 'WARN', 'LOG_FILE': 'log.txt'})        d=runner.crawl(self.convert(body),task_info=json.loads(json.dumps({'taskid': arg["taskid"], 'infoid': arg["infoid"], 'task_name': arg["task_name"]}),object_hook=dict2task),                      spider_context=json.loads(json.dumps({}), object_hook=dict2spdinr))        d=runner.crawl(self.convert(body),task_info=json.loads(json.dumps({'taskid': arg["taskid"], 'infoid': arg["infoid"], 'task_name': arg["task_name"]}),object_hook=dict2task),                      spider_context=json.loads(json.dumps({}), object_hook=dict2spdinr))        runner.start()        print("process",self.process)        d.addBoth(lambda _: self.__spider_exit__(ch, method))    def callback(self, ch, method, properties, body):        # spider_name = task.get("spider_name")        print("body",body)        arg = json.loads(body)        print(arg["taskid"], arg["infoid"], type(arg["taskid"]), type(arg["infoid"]), arg["task_name"],type(arg["task_name"]))        t = multiprocessing.Process(target=self.crawl, args=(body,))        t.start()    def __spider_exit__(self, ch, method):        print("__spider_exit__")        # self.rabbitmq.ack(message_id) #确认消息消费成功 控制并发数        ch.basic_ack(delivery_tag=method.delivery_tag)if __name__ == '__main__':    a = SpiderManager()